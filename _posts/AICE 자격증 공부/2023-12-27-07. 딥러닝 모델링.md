---
title: "07. 딥러닝 모델링"
tags:
 - AICE 자격증 공부
---

## 딥러닝
모델에 입력값을 넣었을 때의 출력값이 최대한 정답과 일치하게 하는 것

- 딥러닝 학습방법

  딥러닝 모델의 매개변수 weight, bias를 무작위로 부여한 후,
  반복학습을 통해 모델의 출력값을 정답과 일치하도록 매개변수를 조금씩 조정한다.
  이때 **Gradient Descent 최적화 알고리즘**을 사용한다.

***

### 딥러닝 기술 원리
- Perceptron

  사람 두뇌에 있는 뉴런을 모델링한 것으로 간단한 함수를 학습할 수 있다.
  Activation Function 활성 함수 사용

- DNN (Deep Neural Network)

  입력층과 출력층 사이에 여러 개의 **은닉층**으로 이루어진 인공신경망으로
  신경망 출력에 **비선형 활성화** 함수를 추가하여
  복잡한 비선형 관계를 모델링 할 수 있다.

- Activation function
  - Binary Step
  - **Logistic, sigmoid, or soft step**
  - Hyperbolic tangent(tanh)
  - **Rectified linear unit(ReLU)**
 
- Loss Function
  
  신경망 학습의 목적함수로 출력값(예측값)과 정답(실제값)의 차이를 계산
  - 회귀 (MSE, MAE)
  - 분류 (이진분류, 다중분류)
    
- Gradient Descent

  뉴럴넷이 가중치 파라미터들을 최적화하는 방법

  손실함수의 현 가중치에서 기울기(Gradient)를 구해서 Loss를 줄이는 방향으로 업데이트 한다.

- Back Propagation

  실제값과 모델 결과값에서 오차를 구해서 오차를 output에서 input 방향으로 보내며 가중치를 재업데이트하면서 학습한다.
  - Forward Propagation

    딥러닝 모델에 값을 입력해서 출력을 얻는 과정
  - Error Back Propagation

    실제값과 모델 결과값에서 오차를 구해서 오차르르 input 방향으로 보내서 가중치를 재업데이트하는 과정

- Optimization Algorithm
  
  딥러닝 모델의 매개변수(Weight, bias)를 조절해서 손실함수의 값을 최저로 만드는 과정

  경사하강법(Gradient Descent)이 대표적이다.

- Data set
  - 1 Epoch: 모든 데이터 셋을 한번 학습
  - 1 iteration: 1회 학습
  - Minibatch: 데이터 셋을 batch size 크기로 쪼개서 학습
 
***

### 딥러닝 알고리즘
- DNN

  이미지 처리를 DNN(Fully Connected Neural Network) 수행 시 문제점
  - 이미지 1차원 형태로 처리:위치 정보 소실 → 정보소실
  - 이미지 사이즈가 커질 경우 비례해서 학습할 가중치 증가
  
  Input layer, Hidden layer1, Hidden layer2

  Dropout 과적합 방지용, Train 학습 시에만 사용

``` Python3
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout

model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(18,)))
model.add(Dense(3, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```
- CNN Convolutional Neural Network
  - 이미지의 특징을 추출하는 부분과 클래스를 분류하는 부분
    - 특징 추출 영역: Convolution Layer와 Pooling Layer를 여러 겹 쌓는 형태로 구성
    - 클래스 분류 영역: Fully Connected Layer 구성하여 사용
  
  - Convolutional Filter
  
    가중치 필터로 학습되며, 이미지 특징/패턴 찾아내 이미지 분류가 가능
  - Padding

    Convolutional layer와 출력 데이터가 줄어드는 것을 방지하는 방법으로
    입력 데이터의 외곽에 0으로 채워 넣는 것이다.

  - Pooling Layer

    계산량, 메모리 사용량, 파라미터 수를 줄이기 위해 입력 이미지의 sub sample(축소본)을 만드는 것으로
    출력 데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용한다.

```Python3
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D

model = Sequential()
model.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=16, kernel_size=3, activation='relu'))
model.add(MaxPooling2D(pool_size=2))

# Classification
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(2, activation='softmax'))
```

- RNN 순환신경망

  입력과 출력을 시퀀스 단위로 처리하는 모델로
  음성 인식, 언어 모델링, 번역, 이미지 주석 생성에 활용한다.

  히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는 인공신경망의 한 종류다.
  
  - one to one
  - one to many
  - many to one
  - many to many
  - many to many

  - LSTM(Long short-Term Memory)
    RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우
    역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 것으로 알려져 있다.
    LSTM이라는 RNN의 히든 state에 cell-state를 추가한 구조로 이용해 오랫동안 정보를 전달하여
    문장과 같은 단어가 문장 안에서의 순서가 중요한 경우나 시계열 데이터 효과를 보여준다.

```Python3
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.layers import LSTM
```
