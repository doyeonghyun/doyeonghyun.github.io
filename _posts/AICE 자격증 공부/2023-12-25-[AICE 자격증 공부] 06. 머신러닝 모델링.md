---
title: "06. 머신러닝 모델링"
tags:
  - AICE 자격증 공부
---
## 머신러닝 모델링
### 머신러닝 프로세스
> 데이터 수집 → 데이터 정리 → (Train) 모델학습 & (Test) 테스트 데이터셋 → 모델테스트 (→ 모델학습으로 돌아가기도 한다) → 모델 배포

### 개념
데이터와 결과를 기반으로 스스로 패턴 학습하고 이를 이용해서 추측

- Linear Regression: 최적 직선 구한다.
- Cost Function = (실제값-예측값)^2/N → MSE(Mean Sqaured Error): 직선별 손실함수 구하기
- Gradient Descent Algorithm: 그래프 상에서 비용이 제일 낮은 아래부분(기울기 0)을 찾아서 손실함수 최소값 찾기

$$
  W:=W-\alpha\frac{\partial}{\partial W}cost(W)\\
$$

***

### 머신러닝 기술 원리
1. 지도 학습
   - 정답을 알려주면서 진행하는 학습으로 데이터와 레이블(정답)이 함께 제공된다.
   - 레이블 = 정답, 실제값, 타깃, 클래스, y
   - 예측된 값 = 예측값, 분류값, y(y hat)
2. 비지도 학습
   - 레이블없이 진행되는 학습으로 데이터 자체에서 패턴을 찾아내야 할 때 사용된다.

***

#### 지도 학습 모델 종류
1. 분류모델
   - 레이블의 값들이 이산적으로 나눠질 수 있는 문제에 사용 (성별 등)
2. 예측모델(회귀모델)
   - 레이블의 값들이 연속적인 문제에 사용(팁)

**데이터셋 구조**
- column:특징 속성(feature), 하나를 선택해서 레이블로 사용
- row:예제(eample) 데이터
- Train(+validation) 7:3 Test

**모델**
- Model(모델), Overfitting(과적합), Underfitting(과소적합)으로 나뉘어 선택한다.
- 모델 성능 평가할 때 train 7:3 test 비율로 진행된다.

```Python3
from sklearn.model_selection import train_test_split
train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
```
- train_test_split(입력, 레이블, test 비율, stratify=비율, random_state)<br/>
  이때 test_size=0.3은 train : test 비율이 7 : 3임을 의미한다.

***

**데이터 정규화/스케일링**
```Python3
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test)
```

1. fit_transform : 훈련 데이터 X_train에 맞춰 학습하고 동시에 훈련 데이터셋을 변환하여 스케일 조정한다.
2. transform : 학습된 scaler 객체를 사용하여 테스트 데이터셋 X_test를 변환한다.

***

**Confusion Matrix**
- 모델의 대략적인 성능확인과 모델의 성능을 오차행렬을 기반으로 수치로 표현

> True Positive(TP): True를 True로 예측 (정답)
>
> True Negative(TN): False를 False로 예측 (정답)
>
> False Positive(FP): False를 True로 예측 (오답)
>
> False Negative(FN): True를 False로 예측 (정답)

**성능지표**
- 정밀도: 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율<br/>
  $$(Precision) = \frac{TP}{TP+FP}$$
- 재현율: 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율<br/>
  $$(Recall) = \frac{TP}{TP+FN}$$
- 정확도: 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가지표
- F1점수:정밀도와 재현율의 조화평균<br/>
  $$(F1-score)=2\times\frac{1}{\frac{1}{Precision}+\frac{1}{recall}}$$

***

### 머신러닝 주요 알고리즘

- Scikit-learn
- 회귀: Linear Regression
- 분류: Logistic Regression(이진 분류에 사용하기 어렵다)
  - 로지스틱 함수
    $$\hat{p}=h_{\theta}(x)=\sigma(x^{T}{\theta})$$<br/>
- 회귀 분류: Decision Tree, Random Forest, K-Nearest Negihbor
- Ensemble 여러 개의 분류기를 생성하고 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
  1. Boosting. 이전 학습에 대하여 잘못 예측된 데이터에 가중치를 부여해 오차를 보완해 나가는 방식
  2. Stacking(Bagging) 여러 개 모델이 예측한 결과 데이터를 기반으로 final_estimator 모델로 종합하여 예측 수행
  3. Weighted Blending. 각 모델의 예측값에 대하여 weight를 곱하여 최종 output 계산

1. Logistic Regression<br/>
   ```
   from sklearn.linear_model import LogisticRegression
   from sklearn.metrics import confusion_matrix 
   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
   from sklearn.metrics import classification_report

   lg = LogisticRegression()
   lg.fit(X_train, y_train)
   lg.score(X_test, y_test)
   lg_pred = lg.predict(X_test)
   confusion_matrix(y_test, lg_pred) #오차행렬
   accuracy_score(y_test, lg_pred) #정확도
   precision_score(y_test, lg_pred) #정밀도
   recall_score(y_test, lg_pred) # 재현율
   f1_score(y_test, lg_pred) #정밀도+재현율
   classfication_report(y_test, lg_pred)
   accuracy_eval('LogisticRegression', lg_pred, y_test)
   ```
2. K-Nearest Neighbor<br/>
   ```
   from sklearn.neighbors import KNeighborsClassifier
   knn = KNeighborsClassificer(n_neighbors=5)
   # fit, predict, accuracy_eval 함수 사용 가능
   ```
3. Decision Tree<br/>
   ```
   from sklearn.tree import DecisionTreeClassifier
   dt = DecisionTreeClassifier(max_depth=10, random_state=42)
   # fit, predict, accuracy_eval 함수 사용 가능
   ```
4. RandomForest<br/>
   ```
   from sklearn.ensemble import RandomForestClassifier
   rfc = RandomForestClassifier(n_estimators=3, random_state=42)
   #fit, predict, accuracy_eval 함수 사용 가능
   ```
5. XGBoost<br/>
   ```
   !pip install xgboost
   from xgboost import XGBClassifier
   xgb = XGBClassifier(n_estimators=3, random_state=42)
   #fit, predict, accuracy_eval 함수 사용 가능
   ```
6. Light GBM<br/>
   ```
   !pip install lightgbm
   from lightgbm import LGBMClassifier
   lgbm = LGBMClassifier(n_estimators=3, random_state=42)
   #fit, predict, accuracy_eval 함수 사용 가능
   ```
7. Stacking<br/>
   ```
   from sklearn.ensemble import StackingRegressor, StackingClassifier
   stack_models = [
    ('LogisticRegression', lg), 
    ('KNN', knn), 
    ('DecisionTree', dt),
   ]
   stacking = StackingClassifier(stack_models, final_estimator=rfc, n_jobs=-1)
   #fit, predict, accuracy_eval 함수 사용 가능
   ```
8. Weighted Blending<br/>
   ```
   final_outputs = {
    'DecisionTree': dt_pred, 
    'randomforest': rfc_pred, 
    'xgb': xgb_pred, 
    'lgbm': lgbm_pred,
    'stacking': stacking_pred,
   }

   final_prediction=\ #weight 곱한다.
   final_outputs['DecisionTree'] * 0.1\
   +final_outputs['randomforest'] * 0.2\
   +final_outputs['xgb'] * 0.25\
   +final_outputs['lgbm'] * 0.15\
   +final_outputs['stacking'] * 0.3\

   final_prediction = numpy.where(final_prediction > 0.5, 1, 0) # 가중치의 합이 1.0이 되도록 한다.
   #accuracy_eval 함수 사용
   ```
